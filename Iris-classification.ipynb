{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This project centered on constructing a robust classifier for Iris species classification using the well-known Iris dataset initially introduced by Ronald A. Fisher. The project spanned various key phases, each playing a pivotal role in enhancing our understanding of the dataset and building an accurate model.\n",
    "\n",
    "the dataset used for this project was sourced from [Kaggle](https://www.kaggle.com/datasets/uciml/iris/data) and found to be relatively clean, requiring only minimal preprocessing. Notably, labels were simplified by removing the \"Iris-\" prefix, and outliers were retained due to the dataset's limited size, containing just 150 entries.\n",
    "\n",
    "The [dimensionality Reduction](#dimensionality-reduction) section explored using various dimensionality reduction techniques, including PCA, t-SNE, and UMAP. While these techniques fell short of creating separate clusters for all three classes, a more focused approach targeting \"versicolor\" and \"virginica\" emerged, using feature elimination to zoom in on petal attributes.\n",
    "\n",
    "The [Model Development and Evaluation](#model-development-and-evaluation) phase involved a two-step model. PCA was employed for dimensionality reduction, followed by SVM for preliminary classification. For cases requiring differentiation between \"versicolor\" and \"virginica,\" feature elimination was applied to focus on petal length and width, with a Logistic Regression model making the final distinction.\n",
    "\n",
    "Model performance was rigorously evaluated through 100 tests involving randomized training and testing data. The results demonstrated robust accuracy, averaging 0.96±0.03 in testing data, reinforcing the model's effectiveness in Iris species classification.\n",
    "\n",
    "In conclusion, This project was successful in building a dependable Iris species classifier. The model's adaptability, combining dimensionality reduction and feature elimination, has proven to be a potent strategy. Beyond this project, the insights gained extend to complex classification problems, offering valuable lessons and reinforcing their applicability in future data science and research endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "0. [Summary](#summary)\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Exploration](#data-exploration)\n",
    "    - [Checking for outliers](#checking-for-outliers)\n",
    "    - [Dimensionality Reduction](#dimensionality-reduction)\n",
    "    - [Versicolor vs. Virginica](#versicolor-vs-virginica)\n",
    "    - [Data Exploration Conclusions](#data-exploration-conclusions)\n",
    "3. [Model Development and Evaluation](#model-development-and-evaluation)\n",
    "4. [Conclusions](#conclusions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Iris dataset, originally introduced by the renowned British biologist and statistician Ronald A. Fisher in 1936, has long served as a fundamental benchmark in the fields of classification and clustering. This dataset, consisting of measurements of Iris flowers' sepal length, sepal width, petal length, and petal width, has become a classic toy problem for data analysis and machine learning. This project's goal is to build an accurate classifier capable of identifying an Iris flower's species with a minimum accuracy threshold of 95%.\n",
    "\n",
    "To achieve our overarching goal of accurate Iris species classification, we have established the following key objectives:\n",
    "\n",
    "1. **Data Collection and Cleaning**: \n",
    "\t- Collect the Iris dataset from a reputable source, ensuring data integrity and completeness.\n",
    "\t- Conduct meticulous data cleaning and preprocessing, addressing any missing values or anomalies that may affect the quality of the data.\n",
    "2. **Preliminary Data Exploration:**\n",
    "\t- Conduct an initial exploration of the Iris dataset to gain insights into its structure and characteristics.\n",
    "\t- Generate summary statistics and visualizations to identify potential issues, patterns, or outliers in the data..\n",
    "3. **Data Preprocessing**:\n",
    "\t- Handle outlier detection and treatment to ensure the dataset is suitable for modeling.\n",
    "\t- Encode categorical variables and handle any other data-specific preprocessing tasks.\n",
    "4. **Feature Engineering**:\n",
    "\t- Identify and select the most relevant features for classification.\n",
    "\t- Implement feature scaling, normalization, or dimensionality reduction techniques to enhance the model's performance.\n",
    "5. **Model Development and Evaluation**:\n",
    "\t- Explore and experiment with various classification models, possibly including ensemble methods, to determine the most suitable approach.\n",
    "\t- Employ rigorous evaluation techniques, including data splitting for training and testing, to assess and fine-tune model performance.\n",
    "\t- Evaluate the models using a range of metrics, including accuracy, precision, recall, F1-score, and confusion matrix, to provide a comprehensive understanding of classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
      "classes: ['setosa', 'versicolor', 'virginica']\n",
      "\t- setosa: 50 entries\n",
      "\t- versicolor: 50 entries\n",
      "\t- virginica: 50 entries\n",
      "Missing values:\n",
      "Id               False\n",
      "SepalLengthCm    False\n",
      "SepalWidthCm     False\n",
      "PetalLengthCm    False\n",
      "PetalWidthCm     False\n",
      "Species          False\n",
      "dtype: bool\n",
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm Species\n",
      "0   1            5.1           3.5            1.4           0.2  setosa\n",
      "1   2            4.9           3.0            1.4           0.2  setosa\n",
      "2   3            4.7           3.2            1.3           0.2  setosa\n",
      "3   4            4.6           3.1            1.5           0.2  setosa\n",
      "4   5            5.0           3.6            1.4           0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/Iris.csv')\n",
    "\n",
    "print(f'columns: {df.columns.tolist()}')\n",
    "\n",
    "df['Species'] = df['Species'].apply(lambda x: x.split('-')[-1]) # removing the `Iris-` prefix from the labels\n",
    "classes = df[\"Species\"].unique().tolist()\n",
    "print(f'classes: {classes}')\n",
    "for species in classes:\n",
    "    n = len(df[df['Species'] == species])\n",
    "    print(f'\\t- {species}: {n} entries')\n",
    "\n",
    "print('Missing values:')\n",
    "print(df.isna().any())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from loading the data in, we see the following characteristics:\n",
    "- There are 150 entries in the dataset\n",
    "- **The dataset has 4 features:** Sepal length (cm), Sepal width (cm), Petal length (cm), and Petal width (cm)\n",
    "- **There are 3 unique classes:** Setosa, Versicolor, and Virginica\n",
    "- there are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for outliers\n",
    "\n",
    "- we must check for outliers in the different classes separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "column_labels = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "colors = ['pink', 'lightblue', 'lightgreen']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True, figsize=(8, 6))\n",
    "fig.suptitle('Boxplots of the different features from different classes')\n",
    "for (i, j), feature, label in zip(product(range(2), range(2)), feature_columns, column_labels):\n",
    "    data = [df[df['Species'] == species][feature].to_numpy() for species in classes]\n",
    "    bplot = ax[i, j].boxplot(data, labels=classes, notch=True, patch_artist=True)\n",
    "    ax[i, j].set_ylabel(f'{label} (cm)')\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set(facecolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using a boxplot, we can immediately see that there are some outliers outside the inter-quartile range (IQR). However, since the dataset is small (only 50 entries per class), the outliers might be explained by the variance in the dataset. Furthermore, while there are outliers, they seem concentrated enough to not interfere with the classification task. Thus, we decide not to eliminate any datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "As a preprocessing step, we can test if the following dimensionality reduction techniques would help the classification task:\n",
    "- Principal component analysis (PCA)\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "First, we need to figure out how much we can reduce the dimensions. This can be done by checking the cumulative explained variance each time we add an additional principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def pca(data, num_components=2):\n",
    "    # Initialize PCA\n",
    "    pca_obj = PCA(n_components=num_components)\n",
    "\n",
    "    # Fit PCA to the standardized data\n",
    "    pca_obj.fit(data)\n",
    "\n",
    "    # Transform the data to the first 'num_components' principal components\n",
    "    data_pca = pca_obj.transform(data)\n",
    "\n",
    "    # Create a DataFrame to visualize the transformed data\n",
    "    df_pca = pd.DataFrame(\n",
    "        data_pca, columns=[f\"PC{i + 1}\" for i in range(num_components)]\n",
    "    )\n",
    "\n",
    "    # Print the explained variance ratio for each principal component\n",
    "    explained_variance_ratios = pca_obj.explained_variance_ratio_\n",
    "\n",
    "    return df_pca, explained_variance_ratios\n",
    "\n",
    "pca_data, explained_variance = pca(df[feature_columns], 4)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "num_of_features = len(feature_columns)\n",
    "for i, exp_var in zip(range(1, num_of_features+1), cumulative_explained_variance):\n",
    "    print(f'principal components: {i}, explained variance: {exp_var:.2f}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_of_features+1), cumulative_explained_variance, 'o-', color='orange')\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that there is a relative improvment from between using one pricipal component and two principal components. For this analysis, we will use 2 principal components simply because there is not much drawbacks in doing so and it would make it easier to visualize the data in a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for species, color in zip(classes, colors):\n",
    "    print()\n",
    "    plt.scatter(pca_data['PC1'][df['Species'] == species], pca_data['PC2'][df['Species'] == species], c=color, label=species)\n",
    "plt.title(\"PCA: First Two Principal Components\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the PCA, we see that there are three distinct clusters. However, it is also evident that the versicolor cluster is a bit too close to the virginica cluster which may impede in the classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "t-SNE is different from PCA in such a way that it is a non-linear dimensionality reduction technique. Thus, we check if it would give better results than PCA in separating the versicolor and virginica clusters.\n",
    "\n",
    "*_Just like what we did in PCA, we reduce the dimensionality of the data into 2 dimensions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(data, n_components=2, perplexity=30.0, n_iter=300, random_state=0):\n",
    "    tsne_model = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, random_state=random_state)\n",
    "    tsne_result = tsne_model.fit_transform(data)\n",
    "    return tsne_result\n",
    "\n",
    "perplexities = [10., 20., 30., 40., 50., 60.]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "\n",
    "for (i, j), perplexity in zip(product(range(2), range(3)), perplexities):\n",
    "    tsne_result = tsne(df[feature_columns], perplexity=perplexity)\n",
    "    for species, color in zip(classes, colors):\n",
    "        ax[i, j].scatter(tsne_result[:, 0][df['Species'] == species], tsne_result[:, 1][df['Species'] == species], c=color, label=species)\n",
    "        ax[i, j].set_title(f'perplexity: {perplexity}')\n",
    "\n",
    "\n",
    "fig.suptitle(\"t-SNE Visualization\")\n",
    "ax[1, 0].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visually checking, it seems like the output of t-SNE with perplexity values 20 or 30 are better compared to using PCA for a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation\n",
    "\n",
    "finally, we can also try using Uniform Manifold Approximation (UMAP) if it would provide better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "n_neighbors = [5, 10, 15, 20, 25, 30]\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "\n",
    "for (i, j), n in zip(product(range(2), range(3)), n_neighbors):\n",
    "    umap_model = umap.UMAP(n_components=2, n_neighbors=n, min_dist=0.05)\n",
    "    umap_result = umap_model.fit_transform(df[feature_columns])\n",
    "    tsne_result = tsne(df[feature_columns], perplexity=perplexity)\n",
    "    for species, color in zip(classes, colors):\n",
    "        ax[i, j].scatter(umap_result[:, 0][df['Species'] == species], umap_result[:, 1][df['Species'] == species], c=color, label=species)\n",
    "        ax[i, j].set_title(f'n_neighbors: {n}')\n",
    "\n",
    "ax[1, 0].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it dosen't seem like UMAP improved the dimensionality reduction much. Additionally, UMAP takes a while to fit to the data so we're probably better off just using t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versicolor vs. Virginica\n",
    "\n",
    "While the dimensionality reduction techniques that we have tested worked at preserving the clusters of datapoints that are in the same class, it had trouble differentiating between versicolor and virginica. Thus, we can isolate the problem and explore ways of differentiating between versicolor and virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Test between the features\n",
    "\n",
    "One method that explore is using a t-test to check each feature and see if there are features of Versicolor that is significantly different from Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "versicolor = df[df['Species'] == 'versicolor']\n",
    "virginica = df[df['Species'] == 'virginica']\n",
    "\n",
    "alpha = 0.05\n",
    "for feature in feature_columns:\n",
    "    t_stat, p_val = ttest_ind(versicolor[feature], virginica[feature], alternative='two-sided')\n",
    "    print(f'feature: {feature}, t-stat: {t_stat:.4f}, p-val: {p_val:.4f}, is significant?: {p_val <= alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the t-tests reveal that all the features are significant. However, notice that the t-statstics for the petal length and width are pretty high compared to the the sepal length and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Elimination\n",
    "\n",
    "From the results in the previous section, it is evident that the t-statistics for the petal length and width are much higher than the sepal length and width. Thus, another avenue that we can explore is eliminating the sepal length and width from the features and classify Versicolor and Virginica using only the petal length and width.\n",
    "\n",
    "Since eliminating `SepalLengthCm` and `SepalWidthCm` would leave us with two features, we can directly use a scatterplot to visualize the clustering of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(versicolor['PetalLengthCm'], versicolor['PetalWidthCm'], c='lightblue', label='versicolor')\n",
    "plt.scatter(virginica['PetalLengthCm'], virginica['PetalWidthCm'], c='lightgreen', label='virginica')\n",
    "plt.xlabel(\"Petal length (cm)\")\n",
    "plt.ylabel(\"petal Width (cm)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation between the clusters are still not absolutely defined. However, this might be good enough and we can just test our model if we could achieve our target accuracy using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Conclusions\n",
    "\n",
    "The Iris dataset contains 150 entries. The dataset did not have any missing values and no data cleaning was required. Furtermore, there are four features describes a single datapoint, namely, `SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, and `PetalWidthCm`. Additionally, there are three distinct classes: `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`. The classes were later renamed into only `setosa`, `versicolor`, and `virginica` -- removing the `Iris-` prefix. While it was identified that there were some outliers in the dataset, removing them was deemed risking since the dataset was small with only 150 entries (50 entries per class).\n",
    "\n",
    "[Dimensionality reduction](#dimensionality-reduction) techniques were also applied to the features of the dataset and it was found that it is possible to reduce the dimensionality of the data into two dimensions while still retaining most of the data's structure. However, it proved difficult to differentiate between [versicolor and virginica](#versicolor-vs-virginica). Using [feature elimination](#feature-elimination) however, to only focus on `PetalLengthCm` and `PetalWidthCm` made it somewhat easier to differentiate between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development and Evaluation\n",
    "\n",
    "Based on the [conclusions](#preliminary-data-exploration-conclusions) in the previous section, a possible model that we can implement is an ensemble model that does the following:\n",
    "1. preprocesses the data by scaling the features\n",
    "2. uses PCA as a preprocessing step \n",
    "3. classifies a datapoint into either `setosa` or `versicolor/virginica` using a Support Vector Machine (SVM).\n",
    "4. if a datapoint is `versicolor/virginica`, it then uses feature reduction to focus on `PetalLengthCm` and `PetalWidthCm`. Otherwise, it outputs the prediction `setosa`.\n",
    "5. after using feature reduction, a logistic regression model will be used to classify between `versicolor` or `virginica`\n",
    "\n",
    "![model flowchart](./images/Iris-classifier.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "class IrisClassifier:\n",
    "\n",
    "    def _pca_fit_transform(self, X):\n",
    "        self._pca_model = PCA(n_components=2)\n",
    "        pca_data = self._pca_model.fit_transform(X.values)\n",
    "        pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "        return pca_df\n",
    "    \n",
    "    def _fit_SVM(self, X, y):\n",
    "        self._svm_model = SVC(kernel='linear')\n",
    "        y = y.apply(lambda x: 'setosa' if x == 'setosa' else 'versicolor/virginica')\n",
    "        self._svm_model.fit(X.values, y.values)\n",
    "\n",
    "    def _fit_logreg(self, X, y):\n",
    "        self._logreg_model = LogisticRegression()\n",
    "        self._logreg_model.fit(X.values, y.values)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_pca = self._pca_fit_transform(X)\n",
    "        self._fit_SVM(X_pca, y)\n",
    "        X_logreg = X[y.isin(['versicolor', 'virginica'])][['PetalLengthCm', 'PetalWidthCm']]\n",
    "        y_logreg = y.loc[X_logreg.index]\n",
    "        self._fit_logreg(X_logreg, y_logreg)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # apply PCA\n",
    "        X_pca = self._pca_model.transform(X)\n",
    "\n",
    "        # classify between setosa or versicolor/virginica\n",
    "        svm_pred = self._svm_model.predict(X_pca)\n",
    "        final_pred = svm_pred\n",
    "\n",
    "\n",
    "        not_setosa = svm_pred != 'setosa'\n",
    "        if not_setosa.any():\n",
    "            # apply dimensionality reduction\n",
    "            X_reduced_features = X[:, -2:]\n",
    "\n",
    "            # classify between versicolor or virginica\n",
    "            logreg_pred = self._logreg_model.predict(X_reduced_features[not_setosa])\n",
    "\n",
    "            # combine results from svm and logreg classifications\n",
    "            final_pred[not_setosa] = logreg_pred\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "def train_test_split(data, test_size=0.2):\n",
    "    data_train = []\n",
    "    data_test = []\n",
    "\n",
    "    for species in df[\"Species\"].unique():\n",
    "        data = df[df[\"Species\"] == species]\n",
    "        train, test = tts(data, test_size=test_size)\n",
    "        data_train.append(train)\n",
    "        data_test.append(test)\n",
    "\n",
    "    data_train = pd.concat(data_train).sample(frac=1)\n",
    "    data_test = pd.concat(data_test).sample(frac=1)\n",
    "\n",
    "    X_train = data_train[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "    y_train = data_train['Species']\n",
    "    X_test = data_test[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "    y_test = data_test['Species']\n",
    "    return X_train, X_test, y_train, y_test\n",
    "        \n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.2 # portion of the data that would be used for testing\n",
    "NUMBER_OF_TRIALS = 100\n",
    "METRICS = ['precision', 'recall', 'f1-score', 'support']\n",
    "\n",
    "train_metrics = np.empty((len(classes), len(METRICS), NUMBER_OF_TRIALS))\n",
    "train_accuracy = np.empty((NUMBER_OF_TRIALS,))\n",
    "test_metrics = np.empty((len(classes), len(METRICS), NUMBER_OF_TRIALS))\n",
    "test_accuracy = np.empty((NUMBER_OF_TRIALS,))\n",
    "\n",
    "for k in range(NUMBER_OF_TRIALS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df)\n",
    "\n",
    "    # initialize model\n",
    "    model = IrisClassifier()\n",
    "\n",
    "    # fit model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate model in training data\n",
    "    train_pred = model.predict(X_train.values)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    train_report = classification_report(y_train, train_pred, output_dict=True)\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        for j, metric in enumerate(METRICS):\n",
    "            train_metrics[i, j, k] = train_report[class_name][metric]\n",
    "    train_accuracy[k] = train_report['accuracy']\n",
    "\n",
    "    # evaluate model in training data\n",
    "    test_pred = model.predict(X_test.values)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    test_report = classification_report(y_test, test_pred, output_dict=True)\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        for j, metric in enumerate(METRICS):\n",
    "            test_metrics[i, j, k] = test_report[class_name][metric]\n",
    "    test_accuracy[k] = test_report['accuracy']\n",
    "\n",
    "# PRINT TRAIN METRICS\n",
    "print('Train metrics')\n",
    "print(f'{\"\":<12}', end='')  # Empty space for the first column header\n",
    "for metric in METRICS:\n",
    "    print(f'{metric:>12}', end='')\n",
    "print()  # End the header line\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f'{class_name:<12}', end='')\n",
    "    for j, metric in enumerate(METRICS):\n",
    "        mean = train_metrics[i, j, :].mean()\n",
    "        stdev = train_metrics[i, j, :].std()\n",
    "        formatted_metric = f'{mean:.2f}±{stdev:.2f}'\n",
    "        print(f'{formatted_metric:>12}', end='')\n",
    "    print()\n",
    "print(f'accuracy: {train_accuracy.mean():.2f}±{train_accuracy.std():.2f}')\n",
    "\n",
    "print('-'*70)\n",
    "\n",
    "# PRINT TEST METRICS\n",
    "print('Test metrics')\n",
    "print(f'{\"\":<12}', end='')  # Empty space for the first column header\n",
    "for metric in METRICS:\n",
    "    print(f'{metric:>12}', end='')\n",
    "print()  # End the header line\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f'{class_name:<12}', end='')\n",
    "    for j, metric in enumerate(METRICS):\n",
    "        mean = test_metrics[i, j, :].mean()\n",
    "        stdev = test_metrics[i, j, :].std()\n",
    "        formatted_metric = f'{mean:.2f}±{stdev:.2f}'\n",
    "        print(f'{formatted_metric:>12}', end='')\n",
    "    print()\n",
    "print(f'accuracy: {test_accuracy.mean():.2f}±{test_accuracy.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "This project has involved a series of critical steps, each contributing to our understanding of the dataset and the development of a robust model:\n",
    "\n",
    "## Data Collection and Cleaning\n",
    "The initial phase involved gathering the Iris dataset from [kaggle](https://www.kaggle.com/datasets/uciml/iris). The data was mostly clean already and didn't have any missing values so cleaning and preprocessing was unnecessary. The only preprocessing step that was deemed necessary was renaming the class labels to remove the `Iris-` prefix for simplicity. While outliers were detected in the dataset, their removal was deemed inappropriate due to the dataset's limited size of 150 entries.\n",
    "\n",
    "## Preliminary Data Exploration\n",
    "Preliminary data exploration shed light on the structure and characteristics of the dataset. It allowed us to generate summary statistics and visualizations, which revealed the presence of distinguishable clusters for the \"setosa\" class. However, challenges emerged in cleanly separating the \"versicolor\" and \"virginica\" classes.\n",
    "\n",
    "## Data Preprocessing\n",
    "Scaling the features is an option that was considered, however using Principal Component Analysis (PCA) in a later step deemed feature scaling uncessary.\n",
    "\n",
    "## Feature Engineering\n",
    "Various dimensionality reduction techniques to reduce the dimensionality of the data into 2 dimensions were tested; namely, PCA, t-distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP). It was discovered that none of these were enough on their own to cleanly separate the three classes, `setosa`, `versicolor`, and `virginica`, into 3 distinct classes. All the techniques have made a distinct cluster for `setosa` but the datapoints for `versicolor` and `virginica` are bunched up with each other. However, it was found that isolating the classification of `versicolor` and `virginica` while using feature elimination to focus on their petal lengths and widths made the classification easier.\n",
    "\n",
    "## Model Development and Evaluation\n",
    "The final model employed a two-step approach. Firstly, we used PCA for dimensionality reduction, which provided two informative features. A Support Vector Machine (SVM) was then utilized to classify datapoints into either \"setosa\" or \"versicolor/virginica.\" If a datapoint was classified as \"setosa,\" it was directly output. In cases where the SVM assigned a datapoint to \"versicolor/virginica,\" feature elimination was employed to focus solely on petal length and width. A Logistic Regression model then distinguished between the two classes.\n",
    "\n",
    "## Model Performance\n",
    "Our model's performance was evaluated through 100 tests, each involving randomization of training and testing data. The results showcased a high level of accuracy in the training data, averaging 0.97±0.01, and a slightly lower but robust **accuracy of 0.96±0.03 in the testing data**. These findings demonstrate the model's ability to effectively classify Iris species, providing accurate results with a small margin of error.\n",
    "\n",
    "In conclusion, this project represents a successful endeavor in the construction of a reliable classifier for Iris species classification. Our model's robust performance in testing data, combined with the use of dimensionality reduction and feature elimination techniques, reinforces the practicality and accuracy of this classification solution. Furthermore, the project offers valuable insights into the relevance of specific features, dimensionality reduction, and the impact of feature elimination in solving complex classification challenges.\n",
    "\n",
    "The knowledge and experience gained from this project not only contribute to the broader field of data science but also offer a solid foundation for addressing similar classification problems and expanding the scope of future research in this domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
